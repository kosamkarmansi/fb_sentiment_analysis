{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSsfW4uGxnW0"
      },
      "source": [
        "\n",
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4zWL8QZWjXC"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6BEKEhvXrI5"
      },
      "source": [
        "fname = 'facebook_comments.csv'\n",
        "df_train = pd.read_csv(fname, header = None, names = ['text', 'sentiment'], encoding='iso-8859-1',lineterminator='\\n')\n",
        "sent = {'positive':2, 'neutral':1,'negative':0}\n",
        "df_train['labels'] = df_train['sentiment'].str.strip().map(sent)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BUMAqCPXzT0",
        "outputId": "c1b4535e-937e-4b52-bc1b-4b66bfb038f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_texts = df_train.text.values #convert to numpy array/vector\n",
        "labels = df_train.labels.values #convert to numpy array/vector\n",
        "\n",
        "print(type(training_texts),type(labels))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vdZoxkwZE33",
        "outputId": "3957ccb6-40e2-42a7-eb93-22886f0aeb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment  labels\n",
              "0  Heres a single  to add  to Kindle. Just read t...    neutral       1\n",
              "1  If you tire of Non-Fiction.. Check out http://...    neutral       1\n",
              "2   Ghost of Round Island is supposedly nonfiction.     neutral       1\n",
              "3  Why is Barnes and Nobles version of the Kindle...   negative       0\n",
              "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiuhnAjbxQ6I"
      },
      "source": [
        "# Pre process data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8CudobHY2dc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVfXCXHIc5B8",
        "outputId": "39e540fd-1216-4506-fde3-17caea893768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=500, ngram_range=(1,2))\n",
        "instances = vectorizer.fit_transform(training_texts) #using TFidf object to tranform the sparse matrix shape\n",
        "X = instances.toarray() # convert the sparse matrix to numpy array\n",
        "Y = labels\n",
        "print(X.shape,',',Y.shape) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 500) , (1999,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cscEZBLt2Qq1"
      },
      "source": [
        "#Traditional Machine Learning Models: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hlS5zv9anS5",
        "outputId": "b8e02e1a-a984-4776-ea71-b781280b932d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle = True, random_state = 2020)\n",
        "rf_model = RandomForestClassifier(criterion = 'entropy', max_depth = 2, random_state = 2020)\n",
        "rf_cvscores = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X):\n",
        "  rf_model.fit(X[train_idx],Y[train_idx]) #train model\n",
        "  acc= rf_model.score(X[val_idx],Y[val_idx]) #get acc from validation set\n",
        "  rf_cvscores.append(acc)\n",
        "\n",
        "print(\"Random Forest - mean: %.4f%% (std: +/- %.4f%%))\" %(np.mean(rf_cvscores)*100, np.std(rf_cvscores)*100))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest - mean: 64.1332% (std: +/- 2.0919%))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnVjOJ_S9TYk"
      },
      "source": [
        "#Fully Connected Feedforward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPgMXEs6c7r_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torch.optim as optim"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJkCBIIFdRrq"
      },
      "source": [
        "epochs = 20\n",
        "lr = 1e-3\n",
        "indim = X.shape[1] #500\n",
        "outdim = 3 #3 categories - positive,negative and neutral\n",
        "drate = 0.5\n",
        "batch_size = 20\n",
        "\n",
        "X_tensor = torch.from_numpy(X) #convert tensor\n",
        "Y_tensor = torch.from_numpy(Y) #convert tensor\n",
        "\n",
        "dataset = TensorDataset(X_tensor,Y_tensor) #convert to dataset of text and labels \n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size = len(dataset)-train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size ,shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# print(len(train_loader), len(train_loader.dataset), (train_loader.batch_size))\n",
        "# print(len(val_loader), len(val_loader.dataset), (val_loader.batch_size))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_vXRSGpmGkA"
      },
      "source": [
        "Build Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCzOpvq7dvth",
        "outputId": "afb2ce05-6c44-431c-c91c-3e2eca94b2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#create model network\n",
        "#input dim - 500\n",
        "class SentimentNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, dropout_rate):\n",
        "    super(SentimentNetwork, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout_rate = dropout_rate\n",
        "    \n",
        "    self.fc1 = nn.Linear(input_dim, 336) #https://www.heatonresearch.com/2017/06/01/hidden-layers.html#:~:text=The%20number%20of%20hidden%20neurons,size%20of%20the%20input%20layer.\n",
        "    self.do1 = nn.Dropout(dropout_rate)\n",
        "    self.fc2 = nn.Linear(336, 336)\n",
        "    self.do2 = nn.Dropout(dropout_rate)\n",
        "    self.fc3 = nn.Linear(336, output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "     x = F.relu(self.fc1(x))\n",
        "     x = F.dropout(self.do1(x))\n",
        "     x = F.relu(self.fc2(x))\n",
        "     x = F.dropout(self.do2(x))\n",
        "     x = F.log_softmax(self.fc3(x))\n",
        "     return x\n",
        "\n",
        "#create model\n",
        "model = SentimentNetwork(indim, outdim, drate)\n",
        "print(model)\n",
        "  \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentNetwork(\n",
            "  (fc1): Linear(in_features=500, out_features=336, bias=True)\n",
            "  (do1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=336, out_features=336, bias=True)\n",
            "  (do2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=336, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8pzXdsfwjdF"
      },
      "source": [
        "Create a training and evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVIhb-jswgAI"
      },
      "source": [
        "def train(model, train_loader, optimizer, criterion):\n",
        "  epoch_loss, epoch_acc = 0.0,0.0 #loss and accuracy for each fn\n",
        "  model.train()\n",
        "\n",
        "  for batch_x, batch_y in train_loader:\n",
        "    batch_x, batch_y = Variable(batch_x),Variable(batch_y)\n",
        "\n",
        "    #print(batch_x.ndimension())\n",
        "    #zero gradient\n",
        "    optimizer.zero_grad()\n",
        "    #prediction = calculate the predicted output for current batch batch_x\n",
        "    prediction = model(batch_x.float())\n",
        "    #loss = calculate the loss for the current batch using predictions and batch_y\n",
        "    loss = criterion(prediction, batch_y)\n",
        "\n",
        "    #convert torch var to numpy: predictions.detach().numpy( )\n",
        "    prediction_numpy = prediction.detach().numpy()\n",
        "\n",
        "    #acc = calculate he accuracy using predictions(batch_size X Output_dim) and batch_y (batch_size X 1)\n",
        "    prediction = prediction.data.max(1)[1]\n",
        "    correct = prediction.eq(batch_y.data).sum().item()\n",
        "    accuracy = correct / (train_loader.batch_size)\n",
        "\n",
        "    #backpropogate\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += accuracy\n",
        "    #calculate avg epoch loss and epoch accuracy\n",
        "    \n",
        "  epoch_loss /= len(train_loader)\n",
        "  epoch_acc /= len(train_loader)\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, criterion):\n",
        "  epoch_loss, epoch_acc = 0.0,0.0 #loss and accuracy for each fn\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_x, batch_y in val_loader:\n",
        "\n",
        "      batch_x, batch_y = Variable(batch_x),Variable(batch_y)\n",
        "\n",
        "      #prediction = calculate the predicted output for current batch batch_x\n",
        "      prediction = model(batch_x.float())\n",
        "\n",
        "      #loss = calculate the loss for the current batch using predictions and batch_y\n",
        "      loss = criterion(prediction, batch_y)\n",
        "\n",
        "      #convert torch var to numpy: predictions.detach().numpy()\n",
        "      prediction_numpy = prediction.detach().numpy()\n",
        "\n",
        "      #acc = calculate he accuracy using predictions(batch_size X Output_dim) and batch_y (batch_size X 1)\n",
        "      prediction = prediction.data.max(1)[1]\n",
        "      correct = prediction.eq(batch_y.data).sum().item()\n",
        "      accuracy = correct / (val_loader.batch_size)\n",
        "\n",
        "      # print(correct, accuracy, len(val_loader.dataset), len(val_loader), correct / len(val_loader.dataset))\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += accuracy\n",
        "\n",
        "\n",
        "    #calculate avg epoch loss and epoch accuracy\n",
        "    epoch_loss /= len(val_loader)\n",
        "    epoch_acc /= len(val_loader)\n",
        "    \n",
        "  return epoch_loss, epoch_acc\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92iwRFC2x1Xw"
      },
      "source": [
        "Train and Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SalbVxpexvt0",
        "outputId": "871dd9d9-8fb8-4dd5-98a4-1ae418c4cbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#loss function and optimizer\n",
        "\n",
        " \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#training and evaluation\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "  print(f'Epoch:{epoch + 1:02}')\n",
        "  print(f'\\tTrain Loss:{train_loss:.4f}| Train Acc: {train_acc:.4f}')\n",
        "  print(f'\\t Val. Loss:{valid_loss:.4f}| Val. Acc: {valid_acc:.4f}')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:01\n",
            "\tTrain Loss:0.8830| Train Acc: 0.6325\n",
            "\t Val. Loss:0.7448| Val. Acc: 0.6575\n",
            "Epoch:02\n",
            "\tTrain Loss:0.6790| Train Acc: 0.6519\n",
            "\t Val. Loss:0.5994| Val. Acc: 0.7550\n",
            "Epoch:03\n",
            "\tTrain Loss:0.5003| Train Acc: 0.8162\n",
            "\t Val. Loss:0.4912| Val. Acc: 0.8175\n",
            "Epoch:04\n",
            "\tTrain Loss:0.3966| Train Acc: 0.8456\n",
            "\t Val. Loss:0.4571| Val. Acc: 0.8300\n",
            "Epoch:05\n",
            "\tTrain Loss:0.3446| Train Acc: 0.8656\n",
            "\t Val. Loss:0.4060| Val. Acc: 0.8350\n",
            "Epoch:06\n",
            "\tTrain Loss:0.2763| Train Acc: 0.8881\n",
            "\t Val. Loss:0.3781| Val. Acc: 0.8575\n",
            "Epoch:07\n",
            "\tTrain Loss:0.2359| Train Acc: 0.9044\n",
            "\t Val. Loss:0.3470| Val. Acc: 0.8800\n",
            "Epoch:08\n",
            "\tTrain Loss:0.2003| Train Acc: 0.9288\n",
            "\t Val. Loss:0.3104| Val. Acc: 0.9050\n",
            "Epoch:09\n",
            "\tTrain Loss:0.1661| Train Acc: 0.9494\n",
            "\t Val. Loss:0.2622| Val. Acc: 0.9300\n",
            "Epoch:10\n",
            "\tTrain Loss:0.1474| Train Acc: 0.9575\n",
            "\t Val. Loss:0.2538| Val. Acc: 0.9375\n",
            "Epoch:11\n",
            "\tTrain Loss:0.1339| Train Acc: 0.9656\n",
            "\t Val. Loss:0.2476| Val. Acc: 0.9350\n",
            "Epoch:12\n",
            "\tTrain Loss:0.1054| Train Acc: 0.9706\n",
            "\t Val. Loss:0.2418| Val. Acc: 0.9300\n",
            "Epoch:13\n",
            "\tTrain Loss:0.0934| Train Acc: 0.9769\n",
            "\t Val. Loss:0.2327| Val. Acc: 0.9300\n",
            "Epoch:14\n",
            "\tTrain Loss:0.0918| Train Acc: 0.9744\n",
            "\t Val. Loss:0.1936| Val. Acc: 0.9450\n",
            "Epoch:15\n",
            "\tTrain Loss:0.0783| Train Acc: 0.9825\n",
            "\t Val. Loss:0.2273| Val. Acc: 0.9475\n",
            "Epoch:16\n",
            "\tTrain Loss:0.0803| Train Acc: 0.9825\n",
            "\t Val. Loss:0.2067| Val. Acc: 0.9600\n",
            "Epoch:17\n",
            "\tTrain Loss:0.0710| Train Acc: 0.9838\n",
            "\t Val. Loss:0.2305| Val. Acc: 0.9475\n",
            "Epoch:18\n",
            "\tTrain Loss:0.0628| Train Acc: 0.9825\n",
            "\t Val. Loss:0.2037| Val. Acc: 0.9550\n",
            "Epoch:19\n",
            "\tTrain Loss:0.0616| Train Acc: 0.9838\n",
            "\t Val. Loss:0.2233| Val. Acc: 0.9475\n",
            "Epoch:20\n",
            "\tTrain Loss:0.0603| Train Acc: 0.9825\n",
            "\t Val. Loss:0.1924| Val. Acc: 0.9625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}